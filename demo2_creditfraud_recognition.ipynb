{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.datafountain.cn/competitions/530"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/doradong/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loan_id', 'user_id', 'total_loan', 'year_of_loan', 'interest', 'monthly_payment', 'class', 'employer_type', 'industry', 'work_year', 'house_exist', 'censor_status', 'issue_date', 'use', 'post_code', 'region', 'debt_loan_ratio', 'del_in_18month', 'scoring_low', 'scoring_high', 'known_outstanding_loan', 'known_dero', 'pub_dero_bankrup', 'recircle_b', 'recircle_u', 'initial_list_status', 'app_type', 'earlies_credit_mon', 'title', 'policy_code', 'f0', 'f1', 'f2', 'f3', 'f4', 'early_return', 'early_return_amount', 'early_return_amount_3mon', 'isDefault']\n",
      "['loan_id', 'user_id', 'total_loan', 'year_of_loan', 'interest', 'monthly_payment', 'class', 'sub_class', 'work_type', 'employer_type', 'industry', 'work_year', 'house_exist', 'house_loan_status', 'censor_status', 'marriage', 'offsprings', 'issue_date', 'use', 'post_code', 'region', 'debt_loan_ratio', 'del_in_18month', 'scoring_low', 'scoring_high', 'pub_dero_bankrup', 'early_return', 'early_return_amount', 'early_return_amount_3mon', 'recircle_b', 'recircle_u', 'initial_list_status', 'earlies_credit_mon', 'title', 'policy_code', 'f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'is_default']\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "train_bank = pd.read_csv('./train_public.csv')\n",
    "train_internet = pd.read_csv('./train_internet.csv')\n",
    "test_bank = pd.read_csv('./test_public.csv')\n",
    "\n",
    "print(list(train_bank))\n",
    "print(list(train_internet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps\n",
    "1. Preprocessing\n",
    "2. Feature Engineering\n",
    "3. Feature Selection\n",
    "4. Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(f'/Users/doradong/pyTools')\n",
    "from data_preprocessing import mem_usage,compress_df,decompress_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre compression: 0 MB\n",
      "aft compression:  0 MB\n",
      "pre compression: 51 MB\n",
      "aft compression:  51 MB\n",
      "pre compression: 0 MB\n",
      "aft compression:  0 MB\n"
     ]
    }
   ],
   "source": [
    "# compress\n",
    "train_bank = compress_df(train_bank, ['int64', 'float64'])\n",
    "train_internet = compress_df(train_internet, ['int64', 'float64'])\n",
    "test_bank = compress_df(test_bank, ['int64', 'float64'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bank.columns = [col.replace('isDefault', 'is_default') for col in train_bank.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['work_year', 'monthly_payment', 'year_of_loan', 'initial_list_status', 'house_exist', 'user_id', 'class', 'f4', 'interest', 'use', 'recircle_b', 'early_return', 'region', 'f3', 'industry', 'earlies_credit_mon', 'issue_date', 'post_code', 'loan_id', 'debt_loan_ratio', 'f1', 'early_return_amount_3mon', 'employer_type', 'early_return_amount', 'f2', 'del_in_18month', 'scoring_high', 'recircle_u', 'is_default', 'pub_dero_bankrup', 'title', 'f0', 'scoring_low', 'total_loan', 'censor_status', 'policy_code']\n"
     ]
    }
   ],
   "source": [
    "common_cols = list(set(list(train_bank)) & set(list(train_internet)))\n",
    "print(common_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The common # of features are:  36\n",
      "public csv contains # of features:  39\n",
      "internet csv contains # of features:  42\n",
      "public csv test contains # of features:  38\n"
     ]
    }
   ],
   "source": [
    "print('The common # of features are: ', len(common_cols))\n",
    "print('public csv contains # of features: ', len(list(train_bank)))\n",
    "print('internet csv contains # of features: ', len(list(train_internet.columns)))\n",
    "print('public csv test contains # of features: ', len(list(test_bank.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public csv features not in common:  ['app_type', 'known_outstanding_loan', 'known_dero']\n",
      "internet csv features not in common:  ['marriage', 'work_type', 'offsprings', 'f5', 'sub_class', 'house_loan_status']\n"
     ]
    }
   ],
   "source": [
    "train_bank_left = list(set(list(train_bank.columns)) - set(common_cols))\n",
    "train_internet_left = list(set(list(train_internet.columns)) - set(common_cols))\n",
    "\n",
    "print('public csv features not in common: ', train_bank_left)\n",
    "print('internet csv features not in common: ', train_internet_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_internet = train_internet[common_cols]\n",
    "train_bank = train_bank[common_cols]\n",
    "test_bank = test_bank[list(set(common_cols)-set(['is_default']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['work_year', 'class', 'industry', 'earlies_credit_mon', 'issue_date',\n",
       "       'employer_type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_internet.select_dtypes(include=['object', 'category']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['post_code', 'region', 'title', 'use', \n",
    "'user_id', 'loan_id', 'is_default', 'issue_date', \n",
    "'policy_code',  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_name_list = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "def split_month(string):\n",
    "    for i in month_name_list:\n",
    "        if i in string:\n",
    "            return month_name_list.index(i)+1 \n",
    "industry_1_list = ['农、林、牧、渔业']\n",
    "industry_2_list = ['制造业', '采矿业', '建筑业', '批发和零售业', '电力、热力生产供应业']\n",
    "industry_3_list = ['金融业', '交通运输、仓储和邮政业', '公共服务、社会组织', '信息传输、软件和信息技术服务业', '房地产业', '文化和体育业', '住宿和餐饮业']\n",
    "def cate_preprocess(df):\n",
    "    # 字段处理\n",
    "    #\"\"\"\n",
    "    df['work_year'] = df['work_year'].apply(str).apply(lambda x: x.split('year')[0].replace('10+', '10').replace('< 1', '0')).apply(np.float)\n",
    "    df['earlies_credit_mon'] = df['earlies_credit_mon'].apply(split_month)\n",
    "    df['is_censor_status_1'] = df['censor_status'].apply(lambda x: 1 if x==1 else 0)\n",
    "    df['is_censor_status_2'] = df['censor_status'].apply(lambda x: 1 if x==2 else 0)\n",
    "    df['is_censor_status_0'] = df['censor_status'].apply(lambda x: 1 if x==0 else 0)\n",
    "    df = df.drop('censor_status', axis=1)\n",
    "    \n",
    "    df['is_industry_1'] = df['industry'].apply(lambda x: 1 if x in industry_1_list else 0)\n",
    "    df['is_industry_2'] = df['industry'].apply(lambda x: 1 if x in industry_2_list else 0)\n",
    "    df['is_industry_3'] = df['industry'].apply(lambda x: 1 if x in industry_3_list else 0)\n",
    "    df = df.drop('industry', axis=1)\n",
    "    \n",
    "    df['is_low_A'] = df['class'].apply(lambda x: 0 if x in ['A'] else 1)\n",
    "    df['is_low_B'] = df['class'].apply(lambda x: 0 if x in ['A', 'B'] else 1)\n",
    "    df['is_low_C'] = df['class'].apply(lambda x: 0 if x in ['A', 'B', 'C'] else 1)\n",
    "    df['is_low_D'] = df['class'].apply(lambda x: 0 if x in ['A', 'B', 'C', 'D'] else 1)\n",
    "    df  = df.drop('class', axis=1)\n",
    "    \n",
    "    df['is_employ_1'] = df['employer_type'].apply(lambda x: 1 if x in ['政府机构', '高等教育机构'] else 0)\n",
    "    df['is_employ_2'] = df['employer_type'].apply(lambda x: 1 if x in ['上市企业', '世界五百强'] else 0)\n",
    "    df['is_employ_3'] = df['employer_type'].apply(lambda x: 1 if x in ['幼教与中小学校', '普通企业'] else 0)\n",
    "    df = df.drop('employer_type', axis=1)\n",
    "    #\"\"\"\n",
    "    \n",
    "    return df\n",
    "train_bank = cate_preprocess(train_bank)\n",
    "train_internet = cate_preprocess(train_internet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_preprocess(df):\n",
    "    # process the time feature\n",
    "    df['issue_date'] = pd.to_datetime(df['issue_date'])\n",
    "    #df['issue_date_y'] = df['issue_date'].dt.year\n",
    "    #df['issue_date_m'] = df['issue_date'].dt.month\n",
    "    \n",
    "    base_time = datetime.datetime.strptime('2007-06-01', '%Y-%m-%d')\n",
    "    df['issue_date_diff'] = df['issue_date'].apply(lambda x: x-base_time).dt.days\n",
    "    df['issue_month_diff'] = df['issue_date'].apply(lambda x: x-base_time).dt.days//30\n",
    "    df['issue_year_diff'] = df['issue_date'].apply(lambda x: x-base_time).dt.days//365\n",
    "    #df[['issue_date', 'issue_date_y', 'issue_date_m', 'issue_date_diff']]\n",
    "    df.drop('issue_date', axis = 1, inplace = True)\n",
    "    \n",
    "    return df\n",
    "train_bank = feature_preprocess(train_bank)\n",
    "train_internet = feature_preprocess(train_internet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_year</th>\n",
       "      <th>monthly_payment</th>\n",
       "      <th>year_of_loan</th>\n",
       "      <th>initial_list_status</th>\n",
       "      <th>house_exist</th>\n",
       "      <th>user_id</th>\n",
       "      <th>f4</th>\n",
       "      <th>interest</th>\n",
       "      <th>use</th>\n",
       "      <th>recircle_b</th>\n",
       "      <th>...</th>\n",
       "      <th>is_low_A</th>\n",
       "      <th>is_low_B</th>\n",
       "      <th>is_low_C</th>\n",
       "      <th>is_low_D</th>\n",
       "      <th>is_employ_1</th>\n",
       "      <th>is_employ_2</th>\n",
       "      <th>is_employ_3</th>\n",
       "      <th>issue_date_diff</th>\n",
       "      <th>issue_month_diff</th>\n",
       "      <th>issue_year_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1174.910034</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240418</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.466</td>\n",
       "      <td>2</td>\n",
       "      <td>7734.230957</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3410</td>\n",
       "      <td>113</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>670.690002</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>225197</td>\n",
       "      <td>22.0</td>\n",
       "      <td>16.841</td>\n",
       "      <td>0</td>\n",
       "      <td>31329.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2192</td>\n",
       "      <td>73</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>603.320007</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>209360</td>\n",
       "      <td>19.0</td>\n",
       "      <td>8.900</td>\n",
       "      <td>4</td>\n",
       "      <td>18514.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2406</td>\n",
       "      <td>80</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   work_year  monthly_payment  year_of_loan  initial_list_status  house_exist  \\\n",
       "0        3.0      1174.910034             3                    0            0   \n",
       "1       10.0       670.690002             5                    1            0   \n",
       "2       10.0       603.320007             3                    1            1   \n",
       "\n",
       "   user_id    f4  interest  use    recircle_b  ...  is_low_A  is_low_B  \\\n",
       "0   240418   4.0    11.466    2   7734.230957  ...         1         1   \n",
       "1   225197  22.0    16.841    0  31329.000000  ...         1         1   \n",
       "2   209360  19.0     8.900    4  18514.000000  ...         0         0   \n",
       "\n",
       "   is_low_C  is_low_D  is_employ_1  is_employ_2  is_employ_3  issue_date_diff  \\\n",
       "0         0         0            1            0            0             3410   \n",
       "1         0         0            1            0            0             2192   \n",
       "2         0         0            1            0            0             2406   \n",
       "\n",
       "   issue_month_diff  issue_year_diff  \n",
       "0               113                9  \n",
       "1                73                6  \n",
       "2                80                6  \n",
       "\n",
       "[3 rows x 47 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bank.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自动分箱、特征筛选\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bank.columns = [col.replace('is_default', 'target') for col in train_bank ]\n",
    "train_internet.columns = [col.replace('is_default', 'target') for col in train_internet ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([train_bank, train_internet])\n",
    "y_train = X_train['target']\n",
    "X_train.drop(['target'],axis=1,inplace=True)\n",
    "\n",
    "X_test = test_bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "def optimal_binning_boundary(x: pd.Series, y: pd.Series, nan: float = -999.) -> list:\n",
    "    '''\n",
    "        obtain optimal boundary based on decision tree\n",
    "    '''\n",
    "    boundary = []  # \n",
    "    \n",
    "    x = x.fillna(nan).values  \n",
    "    y = y.values\n",
    "    \n",
    "    clf = DecisionTreeClassifier(criterion='entropy',    \n",
    "                                 max_leaf_nodes=6,       \n",
    "                                 min_samples_leaf=0.05)  \n",
    "\n",
    "    clf.fit(x.reshape(-1, 1), y)  \n",
    "    \n",
    "    n_nodes = clf.tree_.node_count\n",
    "    children_left = clf.tree_.children_left\n",
    "    children_right = clf.tree_.children_right\n",
    "    threshold = clf.tree_.threshold\n",
    "    \n",
    "    for i in range(n_nodes):\n",
    "        if children_left[i] != children_right[i]:  \n",
    "            boundary.append(threshold[i])\n",
    "\n",
    "    boundary.sort()\n",
    "\n",
    "    min_x = x.min()\n",
    "    max_x = x.max() + 0.1  \n",
    "    boundary = [min_x] + boundary + [max_x]\n",
    "\n",
    "    return boundary\n",
    "def feature_woe_iv(x: pd.Series, y: pd.Series, nan: float = -999.) -> pd.DataFrame:\n",
    "    '''\n",
    "        compute woe & iv \n",
    "    '''\n",
    "    x = x.fillna(nan)\n",
    "    boundary = optimal_binning_boundary(x, y, nan)        \n",
    "    df = pd.concat([x, y], axis=1)                        \n",
    "    df.columns = ['x', 'y']                               \n",
    "    df['bins'] = pd.cut(x=x, bins=boundary, right=False)  \n",
    "    \n",
    "    grouped = df.groupby('bins')['y']                     \n",
    "    result_df = grouped.agg([('good',  lambda y: (y == 0).sum()), \n",
    "                             ('bad',   lambda y: (y == 1).sum()),\n",
    "                             ('total', 'count')])\n",
    "\n",
    "    result_df['good_pct'] = result_df['good'] / result_df['good'].sum()       \n",
    "    result_df['bad_pct'] = result_df['bad'] / result_df['bad'].sum()          \n",
    "    result_df['total_pct'] = result_df['total'] / result_df['total'].sum()    \n",
    "\n",
    "    result_df['bad_rate'] = result_df['bad'] / result_df['total']             \n",
    "    \n",
    "    result_df['woe'] = np.log(result_df['good_pct'] / result_df['bad_pct'])              \n",
    "    result_df['iv'] = (result_df['good_pct'] - result_df['bad_pct']) * result_df['woe']  \n",
    "    \n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv_record=[]\n",
    "woe_df_record = []\n",
    "for feat in X_train.columns:\n",
    "    woe_df_record.append(feature_woe_iv(X_train[feat], y_train, -999))\n",
    "    iv_record.append(woe_df_record[-1]['iv'].sum())\n",
    "    \n",
    "iv_thre = 0.02\n",
    "# keep features that iv>iv_thre\n",
    "iv_record_np = np.array(iv_record)\n",
    "iv_record_index = iv_record_np>=iv_thre\n",
    "X_train = X_train[X_train.columns[iv_record_index]]\n",
    "woe_df_record = np.array(woe_df_record)[iv_record_index]\n",
    "woe_df_record = woe_df_record.tolist()\n",
    "list_record = X_train.columns.tolist()\n",
    "assert len(woe_df_record)==len(X_train.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_woe_data(X_train, woe_df_record):\n",
    "    \"\"\"\n",
    "        substitute ori value with woe \n",
    "    \"\"\"\n",
    "    X_train = X_train.fillna(-999)\n",
    "    for i, col in enumerate(X_train.columns):\n",
    "        bins_tmp = [a.left for a in woe_df_record[i].index.tolist()]+[woe_df_record[i].index.tolist()[-1].right]\n",
    "        X_train[col] = pd.cut(X_train[col], bins=bins_tmp, labels=woe_df_record[i].index.tolist(), right=False, include_lowest=True)\n",
    "        X_train[col] = X_train[col].replace(woe_df_record[i][['woe']].to_dict('dict')['woe'])\n",
    "    return X_train\n",
    "X_train = fill_woe_data(X_train, woe_df_record)\n",
    "X_test = fill_woe_data(X_test[X_train.columns], woe_df_record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features with high pearson value \n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "p_thre = 0.75\n",
    "def my_pearsonr(x_data):\n",
    "    sorce_p_value = []\n",
    "    index_i=0\n",
    "    while index_i<len(x_data.columns)-1:\n",
    "        index_j=index_i+1\n",
    "        while index_j<len(x_data.columns):\n",
    "            p_coeff = pearsonr(x_data[x_data.columns[index_i]], x_data[x_data.columns[index_j]])[0]\n",
    "            if p_coeff>p_thre:\n",
    "                x_data.drop([x_data.columns[index_j]], axis=1, inplace=True)\n",
    "                continue\n",
    "            else:\n",
    "                index_j+=1\n",
    "        index_i+=1\n",
    "    return x_data\n",
    "\n",
    "X_train = my_pearsonr(X_train)\n",
    "woe_df_record = [woe_df_record[i] for i in [list_record.index(a) for a in X_train.columns]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose 20 features with the highest iv\n",
    "feat_num = 20\n",
    "iv_record = {}\n",
    "for i in range(len(woe_df_record)):\n",
    "    iv_record[X_train.columns[i]]=woe_df_record[i]['iv'].sum()\n",
    "\n",
    "iv_list = sorted(iv_record.items(), key=lambda d:d[1], reverse = True)\n",
    "iv_list = iv_list[:feat_num]\n",
    "X_train = X_train[[a[0] for a in iv_list]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data into dev and val sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_dev, X_val, y_dev, y_val = train_test_split(X_train, y_train, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "from sklearn import metrics\n",
    "\n",
    "def ks_score(y_true, y_score):\n",
    "    \"\"\" Calculating the Kolmogorov-Smirnov score\"\"\"\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_true, y_score)\n",
    "    return max(tpr - fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.8006776315789473\n",
      "ks:  0.43066442289261236\n"
     ]
    }
   ],
   "source": [
    "clf_ex=lightgbm.LGBMRegressor(n_estimators = 200)\n",
    "clf_ex.fit(X = X_dev, y = y_dev, verbose=1)\n",
    "#clf_ex.booster_.save_model('LGBMmode.txt')\n",
    "val_pred = clf_ex.predict(X_val)\n",
    "val_pred_list = [int(i) for i in val_pred]\n",
    "print('acc: ', metrics.accuracy_score(y_val, val_pred_list))\n",
    "print('ks: ', ks_score(y_val, val_pred))\n",
    "\n",
    "X_test = X_test[X_train.columns]\n",
    "pred = clf_ex.predict(X_test)\n",
    "pred = [int(i) for i in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission\n",
    "submission = pd.DataFrame({'id':test_bank['loan_id'], 'is_default':pred})\n",
    "submission.to_csv('submission.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Dropout , Lambda, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import  backend as K\n",
    "from keras import models\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "import numpy as np\n",
    "#mean_px = X_train.mean().astype(np.float32)\n",
    "#std_px = X_train.std().astype(np.float32)\n",
    "\n",
    "#X_train_NN =(X_train - mean_px) / std_px\n",
    "#X_test_NN  = (X_test - mean_px) / std_px\n",
    "\n",
    "X_train_NN = (X_train.values).astype('float32')\n",
    "y_train_NN = y_train.astype('int32')\n",
    "\n",
    "X_test_NN = (X_test.values).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 256)               4864      \n",
      "                                                                 \n",
      " layer_normalization_2 (Laye  (None, 256)              512       \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                16448     \n",
      "                                                                 \n",
      " layer_normalization_3 (Laye  (None, 64)               128       \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,017\n",
      "Trainable params: 22,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 修改初始化、加归一层、加dropout、改用不同的metrics\n",
    "seed = 43\n",
    "np.random.seed(seed)\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "def auroc(y_true, y_pred):\n",
    "    return tf.compat.v1.py_func(roc_auc_score, (y_true, y_pred), tf.double)\n",
    "\n",
    "input_shape = X_train_NN.shape[1]\n",
    "b_size = 1024\n",
    "max_epochs = 10\n",
    "\n",
    "import tensorflow.keras as K\n",
    "init = K.initializers.glorot_uniform(seed=1)\n",
    "simple_adam = K.optimizers.Adam(lr=0.01)\n",
    "\n",
    "model = K.models.Sequential()\n",
    "model.add(K.layers.Dense(units=256, input_dim=input_shape, kernel_initializer='he_normal', activation='relu',kernel_regularizer=l2(0.0001)))\n",
    "model.add(K.layers.LayerNormalization())\n",
    "model.add(K.layers.Dropout(0.3))\n",
    "model.add(K.layers.Dense(units= 64, kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(K.layers.LayerNormalization())\n",
    "model.add(K.layers.Dropout(0.3))\n",
    "model.add(K.layers.Dense(units=1, kernel_initializer='he_normal', activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer=simple_adam, metrics=['accuracy',AUC(name='auc')])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting NN training\n",
      "Epoch 1/10\n",
      "594/594 [==============================] - 6s 9ms/step - loss: 0.4307 - accuracy: 0.8087 - auc: 0.7793\n",
      "Epoch 2/10\n",
      "594/594 [==============================] - 6s 10ms/step - loss: 0.4137 - accuracy: 0.8109 - auc: 0.7850\n",
      "Epoch 3/10\n",
      "594/594 [==============================] - 5s 9ms/step - loss: 0.4129 - accuracy: 0.8110 - auc: 0.7855\n",
      "Epoch 4/10\n",
      "594/594 [==============================] - 8s 13ms/step - loss: 0.4130 - accuracy: 0.8109 - auc: 0.7853\n",
      "Epoch 5/10\n",
      "594/594 [==============================] - 13s 23ms/step - loss: 0.4127 - accuracy: 0.8108 - auc: 0.7855\n",
      "Epoch 6/10\n",
      "594/594 [==============================] - 7s 11ms/step - loss: 0.4127 - accuracy: 0.8107 - auc: 0.7854\n",
      "Epoch 7/10\n",
      "594/594 [==============================] - 6s 10ms/step - loss: 0.4124 - accuracy: 0.8105 - auc: 0.7858\n",
      "Epoch 8/10\n",
      "594/594 [==============================] - 9s 15ms/step - loss: 0.4123 - accuracy: 0.8108 - auc: 0.7859\n",
      "Epoch 9/10\n",
      "594/594 [==============================] - 10s 17ms/step - loss: 0.4122 - accuracy: 0.8110 - auc: 0.7858\n",
      "Epoch 10/10\n",
      "594/594 [==============================] - 7s 12ms/step - loss: 0.4124 - accuracy: 0.8108 - auc: 0.7855\n",
      "NN training finished\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting NN training\")\n",
    "h = model.fit(X_train_NN, y_train_NN, batch_size=b_size, epochs=max_epochs, shuffle=True, verbose=1)\n",
    "print(\"NN training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_NN = model.predict(X_test_NN)\n",
    "pred_NN = [item[0] for item in pred_NN]\n",
    "pred_NN = [int(i) for i in pred_NN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('NN_model.h5')\n",
    "submission = pd.DataFrame({'id':test_bank['loan_id'], 'is_default':pred_NN})\n",
    "submission.to_csv('submission.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other tries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(X_dev, y_dev)\n",
    "\n",
    "print(ks_score(y_val, clf.predict_proba(X_val)[:,1]))\n",
    "print(metrics.accuracy_score(y_val, clf.predict(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ks_score(y_val, clf.predict_proba(X_val)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'id':test_bank['loan_id'], 'is_default':clf.predict(X_test)})\n",
    "submission.to_csv('submission.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['is_default'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
